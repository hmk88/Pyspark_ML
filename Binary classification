%sql DROP TABLE IF EXISTS Pyranometer_data

%sql
CREATE TABLE Pyranometer_data (
  temperature DOUBLE,
  solarpower DOUBLE,
  rawdata DOUBLE,
  time STRING,
  pk DOUBLE)
USING CSV
OPTIONS (path "/FileStore/tables/Pyranometer_without_header.csv", header "true")
dataset = spark.table("Pyranometer_data")
cols = dataset.columns
display(dataset)

####Preprocessing the data for ML algorithms 
###Separating data with respect to categorical and numerical columns 

from pyspark.ml import Pipeline
from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler
categoricalColumns = ["time"]
stages = [] # stages in our Pipeline
for categoricalCol in categoricalColumns:
    # Category Indexing with StringIndexer
    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + "Index")
    # Use OneHotEncoder to convert categorical variables into binary SparseVectors
    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + "Index", outputCol=categoricalCol + "classVec")
    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + "classVec"])
    # Add stages.  These are not run here, but will run all at once later on.
    stages += [stringIndexer, encoder]

# Convert label into label indices using the StringIndexer
label_stringIdx = StringIndexer(inputCol="time", outputCol="label")
stages += [label_stringIdx]

# Transform all features into a vector using VectorAssembler
numericCols = ["temperature", "solarpower", "rawdata", "pk"]
assemblerInputs = [c + "classVec" for c in categoricalColumns] + numericCols
assembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features")
stages += [assembler]

##########Applying Algorithms 
from pyspark.ml.classification import LogisticRegression
  
partialPipeline = Pipeline().setStages(stages)
pipelineModel = partialPipeline.fit(dataset)
preppedDataDF = pipelineModel.transform(dataset)

# Fit model to prepped data
lrModel = LogisticRegression().fit(preppedDataDF)

display(lrModel, preppedDataDF)

# Keep relevant columns
selectedcols = ["label", "features"] + cols
dataset = preppedDataDF.select(selectedcols)
display(dataset)

### Randomly split data into training and test sets. set seed for reproducibility
(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)
print(trainingData.count())
print(testData.count())

from pyspark.ml.classification import LogisticRegression

# Create initial LogisticRegression model
lr = LogisticRegression(labelCol="label", featuresCol="features", maxIter=10)

# Train model with Training Data
lrModel = lr.fit(trainingData)

predictions = lrModel.transform(testData)

selected = predictions.select("label", "prediction", "probability", "time", "temperature")
display(selected)

from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Evaluate model
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
evaluator.evaluate(predictions)

evaluator.getMetricName()

print(lr.explainParams())

from pyspark.ml.classification import DecisionTreeClassifier

# Create initial Decision Tree Model
dt = DecisionTreeClassifier(labelCol="label", featuresCol="features", maxDepth=3)

# Train model with Training Data
dtModel = dt.fit(trainingData)

print("numNodes = ", dtModel.numNodes)
print("depth = ", dtModel.depth)

display(dtModel)

predictions = dtModel.transform(testData)

predictions.printSchema()

selected = predictions.select("label", "prediction", "probability", "solarpower", "temperature")
display(selected)

from pyspark.ml.evaluation import BinaryClassificationEvaluator
# Evaluate model
evaluator = BinaryClassificationEvaluator()
evaluator.evaluate(predictions)

dt.getImpurity()

from pyspark.ml.classification import RandomForestClassifier

# Create an initial RandomForest model.
rf = RandomForestClassifier(labelCol="label", featuresCol="features")

# Train model with Training Data
rfModel = rf.fit(trainingData)

predictions = rfModel.transform(testData)

predictions.printSchema()

# View model's predictions and probabilities of each prediction class
selected = predictions.select("label", "prediction", "probability", "temperature", "solarpower")
display(selected)

